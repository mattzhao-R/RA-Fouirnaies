{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir('/c/Users/matth/Documents/Coding/RA Fouirnaies/vs_ltm_linktable/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vote Smart - FollowTheMoney Link Table\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook documents the process of creating a link table between the enterprise/organization ID of Vote Smart and FollowTheMoney. We begin with a list of the desired organizations' names as seen on the Vote Smart website and scrape the FTM website search queries to obtain unique IDs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Query Exploration\n",
    "\n",
    "#### Example: Campaign for Working Families\n",
    "\n",
    "Here we seek to understand the format of the search query output and create a method to extract the desired information from each query while handling/providing discussion of how to handle exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.followthemoney.org/search-results/SearchForm?Search=Campaign+for+Working+Families'\n",
    "headers = {'Access-Control-Allow-Origin': '*',\n",
    "          'Access-Control-Allow-Methods': 'GET',\n",
    "          'Access-Control-Allow-Headers': 'Content-Type',\n",
    "          'Access-Control-Max-Age': '3600'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Issue**\n",
    "\n",
    "Direct parsing of the html using beautifulsoup does not work since the table produced by the search query is a dynamic table, and as a result you need to actually make a request to the website for it to show up.\n",
    "\n",
    "Debugging for dynamic table issue\n",
    "\n",
    "https://stackoverflow.com/questions/17597424/how-to-retrieve-the-values-of-dynamic-html-content-using-python\n",
    "https://stackoverflow.com/questions/40208051/selenium-using-python-geckodriver-executable-needs-to-be-in-path\n",
    "https://www.howtogeek.com/118594/how-to-edit-your-system-path-for-easy-command-line-access/amp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Workaround**\n",
    "\n",
    "Use selenium webdriver (requires firefox and geckodriver) to access the table. This is not ideal since it requires certain downloads/specific file paths in addition to opening the firefox broswer. Based on what I have read, it appears that this is the easiest way to get around the issue of a dynamic table. As a result, we will be using this method to create the link table. I will update the file paths to be generic and add the necessary files to the main directory so that this can be run elsewhere. \n",
    "\n",
    "An additional issue is that each search and window close takes time. As a result, based on the size of the data I will be working with, I will need to utilize parallel execution along with other methods to reduce the runtime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My program took 8.204723119735718 to run\n"
     ]
    }
   ],
   "source": [
    "#driver = webdriver.Firefox() #executable_path=r'your\\path\\geckodriver.exe'\n",
    "#driver.get(url)\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "#from selenium.webdriver.firefox.options import Options\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "#options = Options()\n",
    "#options.binary_location = './Mozilla Firefox/firefox.exe' #r'C:\\Program Files\\Mozilla Firefox\\firefox.exe'\n",
    "#driver = webdriver.Firefox(executable_path='./geckodriver.exe', options=options)\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url)\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, '#dijit_layout_ContentPane_0 > div')))\n",
    "\n",
    "html = driver.page_source\n",
    "\n",
    "driver.close()\n",
    "\n",
    "print(\"My program took\", time.time()-start_time, \"to run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My program took 0.08078145980834961 to run\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#req = requests.get(html, headers)\n",
    "sample = BeautifulSoup(html, 'html.parser') #req.content\n",
    "\n",
    "print(\"My program took\", time.time()-start_time, \"to run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# navigating to dynamic search query table and obtaining a list of the 'Name' element/col of each row\n",
    "query_table = sample.find_all(name='div', class_ = 'table-responsive')\n",
    "query_list = query_table[0].find_all(name='td', style = 'text-align: left;')\n",
    "len(query_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAMPAIGN FOR WORKING FAMILIES: 471\n",
      "CAMPAIGN FOR WORKING FAMILIES PAC CWF: 46277079\n",
      "NEW HAMPSHIRE CAMPAIGN FOR WORKING FAMILIES: 16629025\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vs_name</th>\n",
       "      <th>vs_id</th>\n",
       "      <th>org_name</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Campaign for Working Families</td>\n",
       "      <td>1145</td>\n",
       "      <td>CAMPAIGN FOR WORKING FAMILIES</td>\n",
       "      <td>471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Campaign for Working Families</td>\n",
       "      <td>1145</td>\n",
       "      <td>CAMPAIGN FOR WORKING FAMILIES PAC CWF</td>\n",
       "      <td>46277079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Campaign for Working Families</td>\n",
       "      <td>1145</td>\n",
       "      <td>NEW HAMPSHIRE CAMPAIGN FOR WORKING FAMILIES</td>\n",
       "      <td>16629025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         vs_name vs_id  \\\n",
       "0  Campaign for Working Families  1145   \n",
       "1  Campaign for Working Families  1145   \n",
       "2  Campaign for Working Families  1145   \n",
       "\n",
       "                                      org_name        ID  \n",
       "0                CAMPAIGN FOR WORKING FAMILIES       471  \n",
       "1        CAMPAIGN FOR WORKING FAMILIES PAC CWF  46277079  \n",
       "2  NEW HAMPSHIRE CAMPAIGN FOR WORKING FAMILIES  16629025  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating mock link table\n",
    "\n",
    "vs_name = ['Campaign for Working Families']\n",
    "vs_id = ['1145']\n",
    "names = []\n",
    "ids = []\n",
    "\n",
    "\n",
    "for row in query_list:\n",
    "    names.append(row.string)\n",
    "    ids.append(row['tokenvalue'])\n",
    "    print(row.string + ': ' + row['tokenvalue'])\n",
    "    \n",
    "temp = np.array(vs_name)\n",
    "vs_name = np.repeat(temp,len(query_list))\n",
    "temp = np.array(vs_id)\n",
    "vs_id = np.repeat(temp,len(query_list))\n",
    "\n",
    "    \n",
    "dict = {'vs_name': vs_name, 'vs_id': vs_id, 'org_name': names, 'ID': ids}\n",
    "\n",
    "df = pd.DataFrame(dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Export: Write rows to csv\n",
    "\n",
    "This new information extraction/organization method is meant to be utilized with the parallelized scraping later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "vs_name = 'Campaign for Working Families'\n",
    "vs_id = '1145'\n",
    "for query in query_list:\n",
    "    row = [vs_name, vs_id, query.string, query['tokenvalue']]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Campaign for Working Families',\n",
       " '1145',\n",
       " 'CAMPAIGN FOR WORKING FAMILIES',\n",
       " '471']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "file = open('practice.csv', 'w')\n",
    "writer = csv.writer(file)\n",
    "writer.writerow(['vs_name', 'vs_id', 'ftm_name','ftm_id'])\n",
    "for row in rows:\n",
    "    writer.writerow(row)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search & Match Automation\n",
    "\n",
    "#### Query Creation\n",
    "\n",
    "For both tasks we assume that we are given a list of strings where each string is an organization name that is separated by spaces (if longer than one word) and is of unknown capitalization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.followthemoney.org/search-results/SearchForm?Search=Eagle+Forum+Of+Alabama\n"
     ]
    }
   ],
   "source": [
    "in_string = 'https://justfacts.votesmart.org/interest-group/369/eagle-forum-of-alabama' # sample string from list\n",
    "vs_id = in_string.split(\"/\")[-2]\n",
    "org_name = in_string.split(\"/\")[-1]\n",
    "search_string = \"+\".join(org_name.title().split(\"-\"))\n",
    "query = 'https://www.followthemoney.org/search-results/SearchForm?Search=' + search_string\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Organization Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plan for matching:**\n",
    "\n",
    "0. Check how many queries return more than one organization match\n",
    "1. Start with strict matching i.e. checking if strings are exactly the same and determine how many there are missing\n",
    "2. Depending on the results of (1), look into near matches (utilize split and check how many words match in the list and/or create a specific threshold e.g. x number of words match)\n",
    "3. Based on results of (2), devise a strategy to fill in remaining orgs if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Org Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_names = pd.read_csv('SIG_state.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1617, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs_names.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['state;sig_url'], dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs_names.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_list = vs_names['state;sig_url'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1617"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AL;https://justfacts.votesmart.org/interest-group/369/eagle-forum-of-alabama'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelizing Final Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.guru99.com/sessions-parallel-run-and-dependency-in-selenium.html\n",
    "https://gist.github.com/wooddar/df4c89f381fa20ce819e94782dc5bc04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Runs\n",
    "\n",
    "#### Test Run #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "file = open('draft_table.csv', 'w')\n",
    "writer = csv.writer(file)\n",
    "writer.writerow(['vs_link','vs_name', 'vs_id', 'ftm_name','ftm_id'])\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\matth\\anaconda3\\lib\\threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\matth\\anaconda3\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\matth\\Documents\\Coding\\RA Fouirnaies\\vs_ltm_linktable\\parallelization.py\", line 45, in selenium_queue_listener\n",
      "    selenium_task(worker, current_data)\n",
      "  File \"C:\\Users\\matth\\Documents\\Coding\\RA Fouirnaies\\vs_ltm_linktable\\parallelization.py\", line 26, in selenium_task\n",
      "    function(worker, data)\n",
      "  File \"C:\\Users\\matth\\Documents\\Coding\\RA Fouirnaies\\vs_ltm_linktable\\vs_ltm_linktable.py\", line 73, in scraping\n",
      "    worker.get(query)\n",
      "  File \"C:\\Users\\matth\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\", line 333, in get\n",
      "    self.execute(Command.GET, {'url': url})\n",
      "  File \"C:\\Users\\matth\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\", line 321, in execute\n",
      "    self.error_handler.check_response(response)\n",
      "  File \"C:\\Users\\matth\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\", line 242, in check_response\n",
      "    raise exception_class(message, screen, stacktrace)\n",
      "selenium.common.exceptions.WebDriverException: Message: unknown error: net::ERR_CONNECTION_TIMED_OUT\n",
      "  (Session info: chrome=103.0.5060.114)\n",
      "\n",
      "\n",
      "STOP encountered, killing worker thread\n",
      "STOP encountered, killing worker thread\n",
      "STOP encountered, killing worker thread\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This parallel operation took 4186.534124135971 to run\n"
     ]
    }
   ],
   "source": [
    "from parallelization import parallel\n",
    "from vs_ltm_linktable import scraping\n",
    "\n",
    "parallel(scraping,names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstart_time = time.time()\\nfrom multiprocessing import Queue, cpu_count\\nfrom threading import Thread\\nfrom selenium import webdriver\\nfrom time import sleep\\nfrom numpy.random import randint\\nimport logging\\nfrom vs_ltm_linktable import scraping\\n\\n\\nlogger = logging.getLogger(__name__)\\n\\n# Some example data to pass the the selenium processes, this will just cause a sleep of time i\\n# This data can be a list of any datatype that can be pickled\\n\\nstart = 0\\nend = len(names_list)\\nselenium_data = names_list[start:end]\\nselenium_data.append(\"STOP\")\\n\\n# Create the two queues to hold the data and the IDs for the selenium workers\\nselenium_data_queue = Queue()\\nworker_queue = Queue()\\n\\n# Create Selenium processes and assign them a worker ID\\n# This ID is what needs to be put on the queue as Selenium workers cannot be pickled\\n# By default, make one selenium process per cpu core with cpu_count\\n# TODO: Change the worker creation code to be your webworker of choice e.g. PhantomJS\\nworker_ids = list(range(4)) #changed number of worker_ids from cpu_count() to 4\\nselenium_workers = {i: webdriver.Chrome() for i in worker_ids}\\nfor worker_id in worker_ids:\\n    worker_queue.put(worker_id)\\n\\n\\ndef selenium_task(worker, data):\\n    \"\"\"\\n    This is a demonstration selenium function that takes a worker and data and then does something with the worker and\\n    data.\\n\\n    TODO: change the below code to be whatever it is you want your worker to do e.g. scrape webpages or run browser tests\\n\\n    :param worker: A selenium web worker NOT a worker ID\\n    :type worker: webdriver.XXX\\n    :param data: Any data for your selenium function (must be pickleable)\\n    :rtype: None\\n    \"\"\"\\n    scraping(worker, data)\\n\\n\\ndef selenium_queue_listener(data_queue, worker_queue):\\n    \"\"\"\\n    Monitor a data queue and assign new pieces of data to any available web workers to action\\n\\n    :param data_queue: The python FIFO queue containing the data to run on the web worker\\n    :type data_queue: Queue\\n    :param worker_queue: The queue that holds the IDs of any idle workers\\n    :type worker_queue: Queue\\n    :rtype: None\\n    \"\"\"\\n    logger.info(\"Selenium func worker started\")\\n    while True:\\n        current_data = data_queue.get()\\n        if current_data == \\'STOP\\':\\n            # If a stop is encountered then kill the current worker and put the stop back onto the queue\\n            # to poison other workers listening on the queue\\n            logger.warning(\"STOP encountered, killing worker thread\")\\n            data_queue.put(current_data)\\n            break\\n        else:\\n            logger.info(f\"Got the item {current_data} on the data queue\")\\n        # Get the ID of any currently free workers from the worker queue\\n        worker_id = worker_queue.get()\\n        worker = selenium_workers[worker_id]\\n        # Assign current worker and current data to your selenium function\\n        selenium_task(worker, current_data)\\n        # Put the worker back into the worker queue as  it has completed it\\'s task\\n        worker_queue.put(worker_id)\\n    return\\n\\n\\n# Create one new queue listener thread per selenium worker and start them\\nlogger.info(\"Starting selenium background processes\")\\nselenium_processes = [Thread(target=selenium_queue_listener,\\n                             args=(selenium_data_queue, worker_queue)) for _ in worker_ids]\\nfor p in selenium_processes:\\n    p.daemon = True\\n    p.start()\\n\\n# Add each item of data to the data queue, this could be done over time so long as the selenium queue listening\\n# processes are still running\\nlogger.info(\"Adding data to data queue\")\\nfor d in selenium_data:\\n    selenium_data_queue.put(d)\\n\\n# Wait for all selenium queue listening processes to complete, this happens when the queue listener returns\\nlogger.info(\"Waiting for Queue listener threads to complete\")\\nfor p in selenium_processes:\\n    p.join()\\n\\n# Quit all the web workers elegantly in the background\\nlogger.info(\"Tearing down web workers\")\\nfor b in selenium_workers.values():\\n    b.quit()\\n\\nprint(\"Scraping\", end-start, \"organizations took\", time.time()-start_time, \"to run\")\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "start_time = time.time()\n",
    "from multiprocessing import Queue, cpu_count\n",
    "from threading import Thread\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "from numpy.random import randint\n",
    "import logging\n",
    "from vs_ltm_linktable import scraping\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Some example data to pass the the selenium processes, this will just cause a sleep of time i\n",
    "# This data can be a list of any datatype that can be pickled\n",
    "\n",
    "start = 0\n",
    "end = len(names_list)\n",
    "selenium_data = names_list[start:end]\n",
    "selenium_data.append(\"STOP\")\n",
    "\n",
    "# Create the two queues to hold the data and the IDs for the selenium workers\n",
    "selenium_data_queue = Queue()\n",
    "worker_queue = Queue()\n",
    "\n",
    "# Create Selenium processes and assign them a worker ID\n",
    "# This ID is what needs to be put on the queue as Selenium workers cannot be pickled\n",
    "# By default, make one selenium process per cpu core with cpu_count\n",
    "# TODO: Change the worker creation code to be your webworker of choice e.g. PhantomJS\n",
    "worker_ids = list(range(4)) #changed number of worker_ids from cpu_count() to 4\n",
    "selenium_workers = {i: webdriver.Chrome() for i in worker_ids}\n",
    "for worker_id in worker_ids:\n",
    "    worker_queue.put(worker_id)\n",
    "\n",
    "\n",
    "def selenium_task(worker, data):\n",
    "    \"\"\"\n",
    "    This is a demonstration selenium function that takes a worker and data and then does something with the worker and\n",
    "    data.\n",
    "\n",
    "    TODO: change the below code to be whatever it is you want your worker to do e.g. scrape webpages or run browser tests\n",
    "\n",
    "    :param worker: A selenium web worker NOT a worker ID\n",
    "    :type worker: webdriver.XXX\n",
    "    :param data: Any data for your selenium function (must be pickleable)\n",
    "    :rtype: None\n",
    "    \"\"\"\n",
    "    scraping(worker, data)\n",
    "\n",
    "\n",
    "def selenium_queue_listener(data_queue, worker_queue):\n",
    "    \"\"\"\n",
    "    Monitor a data queue and assign new pieces of data to any available web workers to action\n",
    "\n",
    "    :param data_queue: The python FIFO queue containing the data to run on the web worker\n",
    "    :type data_queue: Queue\n",
    "    :param worker_queue: The queue that holds the IDs of any idle workers\n",
    "    :type worker_queue: Queue\n",
    "    :rtype: None\n",
    "    \"\"\"\n",
    "    logger.info(\"Selenium func worker started\")\n",
    "    while True:\n",
    "        current_data = data_queue.get()\n",
    "        if current_data == 'STOP':\n",
    "            # If a stop is encountered then kill the current worker and put the stop back onto the queue\n",
    "            # to poison other workers listening on the queue\n",
    "            logger.warning(\"STOP encountered, killing worker thread\")\n",
    "            data_queue.put(current_data)\n",
    "            break\n",
    "        else:\n",
    "            logger.info(f\"Got the item {current_data} on the data queue\")\n",
    "        # Get the ID of any currently free workers from the worker queue\n",
    "        worker_id = worker_queue.get()\n",
    "        worker = selenium_workers[worker_id]\n",
    "        # Assign current worker and current data to your selenium function\n",
    "        selenium_task(worker, current_data)\n",
    "        # Put the worker back into the worker queue as  it has completed it's task\n",
    "        worker_queue.put(worker_id)\n",
    "    return\n",
    "\n",
    "\n",
    "# Create one new queue listener thread per selenium worker and start them\n",
    "logger.info(\"Starting selenium background processes\")\n",
    "selenium_processes = [Thread(target=selenium_queue_listener,\n",
    "                             args=(selenium_data_queue, worker_queue)) for _ in worker_ids]\n",
    "for p in selenium_processes:\n",
    "    p.daemon = True\n",
    "    p.start()\n",
    "\n",
    "# Add each item of data to the data queue, this could be done over time so long as the selenium queue listening\n",
    "# processes are still running\n",
    "logger.info(\"Adding data to data queue\")\n",
    "for d in selenium_data:\n",
    "    selenium_data_queue.put(d)\n",
    "\n",
    "# Wait for all selenium queue listening processes to complete, this happens when the queue listener returns\n",
    "logger.info(\"Waiting for Queue listener threads to complete\")\n",
    "for p in selenium_processes:\n",
    "    p.join()\n",
    "\n",
    "# Quit all the web workers elegantly in the background\n",
    "logger.info(\"Tearing down web workers\")\n",
    "for b in selenium_workers.values():\n",
    "    b.quit()\n",
    "\n",
    "print(\"Scraping\", end-start, \"organizations took\", time.time()-start_time, \"to run\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Validation\n",
    "\n",
    "#### Checking Organizations\n",
    "\n",
    "Check the number of organizations sucessfully scraped and gaps in matching, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_csv(\"temp_draft_table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1582"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp['vs_name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3173, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vs_link     object\n",
       "vs_name     object\n",
       "vs_id        int64\n",
       "ftm_name    object\n",
       "ftm_id      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "641"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[temp['ftm_name']=='TimeoutException'].shape[0] # number of scrapes that ran into timeoutexception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "540"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp[temp['ftm_name']=='TimeoutException']['vs_link'].unique()) # numbers don't match but in theory should because\n",
    "# when an error occurs, the function should output a row with timeoutexception filled in for values once and then exit and move on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp[temp['ftm_name']=='ResultsEmpty']['vs_link'].unique()) # number of scrapes where a table showed up but there were no matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rerun = temp[temp['ftm_name']=='TimeoutException']['vs_link'].tolist()\n",
    "type(rerun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parallelization import parallel\n",
    "from vs_ltm_linktable import scraping\n",
    "\n",
    "parallel(scraping,rerun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
